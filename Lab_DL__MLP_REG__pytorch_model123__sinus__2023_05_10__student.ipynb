{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJE_YM1Uy3zH"
      },
      "source": [
        "# Lab Deep Learning / Multi-Layer Perceptron for regression / in pytorch\n",
        "\n",
        "**Author: geoffroy.peeters@telecom-paris.fr**\n",
        "\n",
        "For any remark or suggestion, please feel free to contact me.\n",
        "\n",
        "Last edits:\n",
        "- 2020/03/24 peeters (change dimensions order)\n",
        "- 2020/04/27 peeters (added comments on W1, W2 initialization, added comments on A and B)\n",
        "- 2021/05/05 peeters (add some documentation related to ```W1 -= ...```)\n",
        "- 2023/05/10 peeters (typos corrections)\n",
        "- 2024/05/02 peeters (link to do nn.Linear)\n",
        "\n",
        "Read the \"introduction to pytorch\"\n",
        "- slides: https://perso.telecom-paristech.fr/gpeeters/doc/pytorch/\n",
        "- video: https://perso.telecom-paristech.fr/gpeeters/video/pytorch.mp4\n",
        "\n",
        "\n",
        "## Objective:\n",
        "\n",
        "The objective of this lab is to demonstrate the use of Neural Networks to perform non-linear regression.\n",
        "We will use a simple NN with 1 hidden layer. The hidden layer has $n_h$ (you will test various values of $n_h$) hidden units and either a ```sigmoid```, ```relu``` or ```tanh``` activation.\n",
        "Since we perform a regression task the output $\\hat{y} \\in \\mathbb{R}$ and  there is no output activation (linear).\n",
        "\n",
        "You will perform 1000 iterations (epochs) of SGD to find the parameters.\n",
        "\n",
        "You will then apply the network to the two datasets A and B.\n",
        "\n",
        "Note: for this lab, we do not separate the dataset into a train, validation and test part. We simply check the ```capacity``` of the network to model the training data.\n",
        "\n",
        "**Question**: according to the obtained loss, discuss the choice of ```sigmoid```, ```relu``` or ```tanh```.\n",
        "\n",
        "\n",
        "\n",
        "### Data normalization\n",
        "\n",
        "You should normalize the data to zero mean and unit standard deviation\n",
        "\n",
        "### Model\n",
        "\n",
        "There are various ways to write a NN model in pytorch.\n",
        "\n",
        "In this lab, you will write three different implementations:\n",
        "- **Model A**: manually defining the parameters (W1,b1,w2,b2), writting the forward equations, writting the loss equation, calling the .backward() and manually updating the weights using W1.grad. You will write the loop to perform 1000 epochs.\n",
        "- **Model B**: using the Sequential class of pytorch\n",
        "- **Model C**: a custom torch.nn.Module class for this.\n",
        "\n",
        "For Model B and C, you will use the ready made loss and optimization from the nn and optim packages. You can use the same code to optimize the parameters of Model B and C.\n",
        "\n",
        "### Loss\n",
        "\n",
        "Since we are dealing with a regression problem, we will use a Mean Square Error loss: write it by-hand for Model A and use ```torch.nn.MSELoss``` for Model B and C.\n",
        "\n",
        "### Parameters update/ Optimization\n",
        "\n",
        "For updating the parameters, we will use as optimizer a simple SGD algorithm (use ```torch.optim.SGD``` for Model B and C) with a learning rate of 0.05.\n",
        "\n",
        "Don't forget that an optimizer is applied to a set of parameters (```my_model.parameters()``` gives the parameters of the network for Model B and C).\n",
        "Once the gradients have been computed (after the backpropagation has been performed), you can perform one step of optimization (using ```optimizer.step()``` for Model B and C).\n",
        "\n",
        "### Backward propagation\n",
        "\n",
        "Backpropagation is automatically performed in pytorch using the ```autograd``` package.\n",
        "First, reset the gradients of all parameters (using ```optimizer.zero_grad()``` for Model B and C), then perform the backpropagation ```loss.backward()```.\n",
        "\n",
        "### Regularization\n",
        "\n",
        "- How can you add a Ridge (L2) regularization ? this can be done easely within the definition of the ```optimizer```.\n",
        "- How can you add a Lasso (L1) regularization ? this has to be done manually at each iteration, by adding something to the loss before backpropagation.\n",
        "\n",
        "**Question**: discuss the use of the Ridge and Lasso regularization for each dataset.\n",
        "\n",
        "## Your task:\n",
        "\n",
        "You need to add the missing parts in the code (parts between ```# --- START CODE HERE``` and ```# --- END CODE HERE```)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILpTZTCEy3zL"
      },
      "source": [
        "## Load the python packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2r45NeUGy3zM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rf4SmJPky3zN"
      },
      "source": [
        "## Datasets\n",
        "\n",
        "We will use two different datasets. For both $x$ has dimensions $(m,n_{in}=1)$ and $y$ has dimensions $(m,n_{out}=1)$ where $m$ is the number of examples.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QO3KZbUC9Tdo"
      },
      "outputs": [],
      "source": [
        "def F_do_dataset_parab(m):\n",
        "    x_data = torch.linspace(-1, 1, m)\n",
        "    noise_data = 0.2*torch.rand(m)\n",
        "    y_data = x_data.pow(2) + noise_data\n",
        "\n",
        "    # --- change vectos to matrix\n",
        "    x_data = torch.unsqueeze(x_data, dim=1)\n",
        "    y_data = torch.unsqueeze(y_data, dim=1)\n",
        "    return x_data, y_data\n",
        "\n",
        "def F_do_dataset_sin(m):\n",
        "    x_data = torch.arange(0, m, dtype=torch.float32)\n",
        "    noise_data = 0.2*torch.rand(m)\n",
        "    y_data = torch.sin(2*np.pi*x_data*0.001) + noise_data\n",
        "\n",
        "    # --- change vectos to matrix\n",
        "    x_data = torch.unsqueeze(x_data, dim=1)\n",
        "    y_data = torch.unsqueeze(y_data, dim=1)\n",
        "    return x_data, y_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "id": "IsT5JftB9XAo",
        "outputId": "203fcfa5-ce8c-4934-d9db-9d32f5dbaec2"
      },
      "outputs": [],
      "source": [
        "# --- Dataset 1\n",
        "#x_data, y_data = F_do_dataset_parab(m=100)\n",
        "# --- Dataset 2\n",
        "x_data, y_data = F_do_dataset_sin(m=3000)\n",
        "\n",
        "plt.plot(x_data.numpy(), y_data.numpy());\n",
        "plt.xlabel('x'), plt.ylabel('y');\n",
        "\n",
        "print('size of x_data:', x_data.size())\n",
        "print('size of y_data:', y_data.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxziZXvgy3zP"
      },
      "source": [
        "## Data standardization\n",
        "\n",
        "We standardize the input data (substract mean and divide by standard-deviation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLCSmUcdy3zP"
      },
      "outputs": [],
      "source": [
        "# --- START CODE HERE\n",
        "x_data = ...\n",
        "y_data = ...\n",
        "# --- END CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_B377lBly3zQ"
      },
      "source": [
        "## Hyper-parameters definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kuK5jqvy3zQ"
      },
      "outputs": [],
      "source": [
        "n_in = 1\n",
        "n_h = 10\n",
        "n_out = 1\n",
        "\n",
        "nb_epoch = 20000\n",
        "learning_rate_alpha = 0.05\n",
        "param_momentum = 0.01\n",
        "param_L1_weight = 0.001\n",
        "param_L2_weight = 0.01"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MW9APEky3zS"
      },
      "source": [
        "## Model A (writting the network equations)\n",
        "\n",
        "For model A, you will define the variables and write the equations of the network yourself (as you would do in numpy).\n",
        "However you will use ```torch tensors``` instead of ```numpy array```.\n",
        "\n",
        "***Why ?*** because torch tensors will allows you to automatically get the gradient. You will use ```loss.backward()``` to launch the backpropagation from the ```loss``` variable.\n",
        "Then, for all torch tensors you have created and for which you have declared ```requires_grad=True```, you will get the gradient of ```loss```with respect to this variable in the field ```.grad```.\n",
        "\n",
        "***Example*** ```W1 = torch.tensors(..., requires_grad=True)``` ... ```loss.backward()``` will have the gradient $\\frac{d Loss}{d W1}$in ```W1.grad```.\n",
        "\n",
        "### Initialize the variables\n",
        "\n",
        "**Note**: In order to avoid neuron saturation and that all the neurons learn the same thing, the matrix should be initialized to a small random value (see https://pytorch.org/docs/stable/generated/torch.nn.Linear.html for details). Because of this, W1 and W2 should be first define as small random; **then** set to ```requires_grad=True```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPdqXBGRy3zT"
      },
      "outputs": [],
      "source": [
        "# --- START CODE HERE\n",
        "W1 = ...\n",
        "b1 = ...\n",
        "W2 = ...\n",
        "b2 = ...\n",
        "# --- END CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYB5Bg5Xy3zT"
      },
      "source": [
        "### Define the model\n",
        "\n",
        "We will denote by ```H``` the pre-activation (value before the non-linearity) and by ```A``` the activation (value after the non-linearity)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIE2mjviy3zT"
      },
      "outputs": [],
      "source": [
        "def model(X):\n",
        "\n",
        "    # --- H is the results of the first projection\n",
        "    # --- H is then transformed by a non-linearity to A\n",
        "\n",
        "    # --- START CODE HERE\n",
        "    ...\n",
        "    # --- END CODE HERE\n",
        "    return y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ax8RL3jYy3zU"
      },
      "source": [
        "### Train the model\n",
        "\n",
        "**Note**: when updating the parameters (W1,b1,W2,b2) you should use an \"in-place\" update: ```W1 -= ...``` instead of ```W1 = W1 - ...```; otherwise ```W1``` will be replaced by its value and its gradient field will not exist anymore."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Klrkf4hovpfS"
      },
      "outputs": [],
      "source": [
        "learning_rate_alpha=0.05"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqOT9eDfy3zU",
        "outputId": "0888c420-dda1-4004-81ae-7e9ed88735c5"
      },
      "outputs": [],
      "source": [
        "for epoch in range(0, nb_epoch):\n",
        "    # --- X (m, n_in)\n",
        "    # --- Y (m, n_out)\n",
        "    # --- START CODE HERE\n",
        "    ...\n",
        "    # --- END CODE HERE\n",
        "\n",
        "    if epoch % 1000 == 0:\n",
        "        print('epoch {}, loss {}'.format(epoch, loss))\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # --- START CODE HERE\n",
        "        ...\n",
        "        # --- END CODE HERE\n",
        "        W1.grad.zero_()\n",
        "        b1.grad.zero_()\n",
        "        W2.grad.zero_()\n",
        "        b2.grad.zero_()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_0Olwgby3zU"
      },
      "source": [
        "### Plotting the results\n",
        "\n",
        "We will super-impose the biases of the first layer to better understand how the model has fit the sinusoidal curve. Please explain what it does."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "D7gfGsbVy3zU",
        "outputId": "29fe20d6-d152-4726-ad51-129213fa6017"
      },
      "outputs": [],
      "source": [
        "# ----------------\n",
        "plt.plot(x_data.numpy(), y_data.numpy(), 'g.');\n",
        "plt.plot(x_data.numpy(), y_pred.detach().numpy(), 'r.');\n",
        "# super-impose the biases of the first layer\n",
        "plt.plot(b1.detach().numpy(), np.zeros((10,1)), 'b+');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FABrUpuQom_P"
      },
      "source": [
        "## Model B (using Sequential class)\n",
        "\n",
        "Here, you will write the network using the nn.Sequential class.\n",
        "With this you can use the ready-made layers ``torch.nn.Linear``,``torch.nn.Tanh``, ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etd99ZJYy3zV"
      },
      "outputs": [],
      "source": [
        "# --- START CODE HERE\n",
        "...\n",
        "# --- END CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sF17zNiYy3zV"
      },
      "source": [
        "## Model C (using a class definition)\n",
        "\n",
        "Here, you will write the network using the recommended pytorch way; i.e. by defining a class.\n",
        "This class inherit from the main class ```torch.nn.Module```.\n",
        "You only need to write the ```__init__``` method and the ```forward``` method.\n",
        "\n",
        "In object programming, the ```__init__``` method defines the attributes of your class. Since the attributes of your  network are the parameters to be trained (weights and biases), you should declare in the ```__init``` all the layers that involve parameters to be trained (mostly the ```Linear```layers which perform the matrix multiplication).\n",
        "\n",
        "The ```forward``` method contains the code of the forward pass itself. It can of course call attributes defined in the ```__init___``` method. It is the method used when calling ```model(x)```.\n",
        "\n",
        "As before, the model created will have all its parameters accessible as a dictionary and can be accessed using ```model.parameters()```.\n",
        "\n",
        "Classes are convenient way to write more complex network than what you can do with ```nn.sequential```. Note that you can actually include a ```nn.sequential``` in your class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mvh3WWGWy3zV"
      },
      "outputs": [],
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        # --- START CODE HERE\n",
        "        ...\n",
        "        # --- END CODE HERE\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # --- START CODE HERE\n",
        "        ...\n",
        "        # --- END CODE HERE\n",
        "\n",
        "        return y_pred\n",
        "\n",
        "# --- START CODE HERE\n",
        "...\n",
        "# --- END CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgIEgKkQBByo"
      },
      "source": [
        "## Model B+C"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QN99zeaLy3zV"
      },
      "source": [
        "### Criterion and Optimization\n",
        "\n",
        "The code of Model A is self-contained, i.e. it already contains all necessary instruction to perform forward, loss, backward and parameter updates.\n",
        "\n",
        "When using ```nn.sequential``` (model B) or a class definition of the network (model C), we still need to define\n",
        "- what we will minimize (the loss to be minimized, i.e. Mean-Square-Error or Binary-Cross-Entropy). We can of course write the equation of it by hand but pytorch comes with a very large number of pre-build loss functions (within ```torch.nn```)\n",
        "- how we will minimize the loss, i.e. what parameter update alogirhtms we will use (SGD, momentum). We can of course write the equation of it by hand but pytorch comes with a very large number of pre-build loss functions (within ```torch.nn```)\n",
        "\n",
        "L2 regularization can be done directly in the optimizer under the name ```weight_decay```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCLVIUQuy3zW"
      },
      "outputs": [],
      "source": [
        "# --- START CODE HERE\n",
        "...\n",
        "# --- END CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlwUVVRFy3zW"
      },
      "source": [
        "### Training\n",
        "\n",
        "Having defined the network, the citerion to be minimized and the optimizer, we then perform a loop over epochs (iterations); at each step we\n",
        "- compute the forward pass by passing the data to the model: ```haty = model(x)```\n",
        "- compute the the loss (the criterion)\n",
        "- putting at zero the gradients of all the parameters of the network (this is important since, by default, pytorch accumulate the gradients over time)\n",
        "- computing the backpropagation (using as before ```.backward()```)\n",
        "- performing one step of optimization (using ```.step()```)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXr0DCiAy3zW",
        "outputId": "2e3a915b-d5e2-44c1-8ed4-f076fc7d05de"
      },
      "outputs": [],
      "source": [
        "loss_l = []\n",
        "for epoch in range(nb_epoch):\n",
        "\n",
        "    # --- START CODE HERE\n",
        "    ...\n",
        "    # --- END CODE HERE\n",
        "\n",
        "    loss_l.append(loss)\n",
        "    if epoch % 1000 == 0:\n",
        "        print('epoch {}, loss {}'.format(epoch, loss.data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "kCLojfiwwpo2",
        "outputId": "a1238d87-bc53-4b77-bdb6-ddfb32106cc3"
      },
      "outputs": [],
      "source": [
        "# ----------------\n",
        "plt.plot([loss.detach().numpy() for loss in loss_l]);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtQKt0DQy3zW"
      },
      "source": [
        "### Plotting results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "f8o2fdZly3zW",
        "outputId": "b707fa81-5071-4ac2-8e55-69030c809330"
      },
      "outputs": [],
      "source": [
        "# ----------------\n",
        "plt.plot(x_data.numpy(), y_data.numpy(), 'g')\n",
        "plt.plot(x_data.numpy(), y_pred.detach().numpy(), 'r')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0CIuaGCy3zW"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLVJhha3y3zX"
      },
      "source": [
        "### Display the weights and the biases\n",
        "\n",
        "Now, we want to check the effect of L1 regularization. For this you will plot the values of the weights of each layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LaeyVcXTy3zX"
      },
      "outputs": [],
      "source": [
        "list_l = [{'name':name, 'data':layer.data} for name, layer in model.named_parameters()]\n",
        "for tmp in list_l:\n",
        "    plt.figure()\n",
        "    plt.plot(tmp['data'].flatten().numpy(), '.')\n",
        "    plt.title(tmp['name'])\n",
        "    plt.grid(True)\n",
        "    print(tmp['data'].numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxphPTywy3zX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
